<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation.">
  <meta name="keywords" content="Long-Horizon Language-Conditioned Manipulation, Large Language Models, Vision-Langage Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LoHoRavens</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4Y34PZ3XBE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4Y34PZ3XBE');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("single-task-result-video");
      video.src = "https://cliport.github.io/media/results_web/" + 
                  task + 
                  "-two_stream_full_clip_lingunet_lat_transporter-n" + 
                  demo + 
                  "-train/videos/" + 
                  task +
                  "-0000" + 
                  inst + 
                  ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }

    function updateMultiVideo() {
      var demo = document.getElementById("multi-menu-demos").value;
      var task = document.getElementById("multi-menu-tasks").value;
      var inst = document.getElementById("multi-menu-instances").value;

      console.log("multi", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "https://cliport.github.io/media/results_web/" + 
                  task + 
                  "-two_stream_full_clip_lingunet_lat_transporter-n" + 
                  demo + 
                  "-train/videos/multi-language-conditioned-" + 
                  task +
                  "-0000" + 
                  inst + 
                  ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }

  </script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://mohitshridhar.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://askforalfred.com/">
            ALFRED
          </a>
          <a class="navbar-item" target="_blank" href="http://alfworld.github.io/">
            ALFWorld
          </a>
          <a class="navbar-item" target="_blank" href="https://arxiv.org/pdf/1806.03831.pdf">
            INGRESS
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation</h1>
          <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">CoRL 2021</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://shengqiang-zhang.github.io/">Shengqiang Zhang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.phil-wicke.com/">Philipp Wicke</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.ch/citations?user=w5ePE1oAAAAJ&hl=en">Lütfi Kerem Şenel</a><sup>1,3</sup>,</span><br>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=ppZN58sAAAAJ&hl=en">Luis Figueredo</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=IBzbBbwAAAAJ&hl=en">Abdeldjallil Naceri</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.de/citations?user=H1v0ztEAAAAJ&hl=de">Sami Haddadin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://bplank.github.io/">Barbara Plank</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=qIL9dWUAAAAJ&hl=en">Hinrich Schütze</a><sup>1,3</sup>,</span>
            
            
            <!-- <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1, 2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CIS, LMU Munich,</span>
            <span class="author-block"><sup>&nbsp;2</sup>RSI, MIRMI, TUM</span><br>
            <span class="author-block"><sup>3</sup>Munich Center for Machine Learning (MCML)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/pdf/2109.12098.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/1xzG5e1XF958HPuD_FZTiKROd9AQyd1fS/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://youtu.be/sguEFlVdEUA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/Shengqiang-Zhang/LoHo-Ravens"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop height="100%">
        <source src="https://cliport.github.io/media/videos/10sim_web_teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </br>
        <span class="dcliport">CLIPort</span> is an end-to-end imitation-learning agent that can learn a single language-conditioned policy for various tabletop tasks.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/1_folding.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/4_chess.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/3_packing.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/6_sweeping.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/9_cherry.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/7_reading.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/2_bowl.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/8_rope.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/5_stacking.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  We learn <b>one multi-task policy</b> for 9 real-world tasks including folding cloths, sweeping beans etc. with just <b>179</b> image-action training pairs.
</h2> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered" style="position:relative">
      <div class="column is-four-fifths" style="float: left; width: 80%;">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following.
            Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations.
            However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. 
            To fill this gap, this work focuses on the tabletop
            manipulation task and releases a simulation benchmark,
            <i>LoHoRavens</i>, which covers various long-horizon
            reasoning aspects spanning color, size, space, arithmetics
            and reference.
            Furthermore, there is a key modality bridging problem for
            long-horizon manipulation tasks with LLMs: how to
            incorporate the observation feedback during robot execution
            for the LLM's closed-loop planning, which is however less studied by prior work. 
            We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively.
            These methods serve as the two baselines for our proposed benchmark. 
            Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models.
            We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks. 
          </p>
        </div>
      </div>
      <div class="column is-four-fifths" style="position:absolute; bottom:0; width: 40%; float: right">
        <!-- <div class="float-bottom"> -->
        <img src="https://shengqiang-zhang.github.io/lohoravens-webpage/media/images/firstpage.png" class="interpolation-image" 
        alt="Interpolate start reference image." />
        <!-- </div> -->
      </div>
    </div>
    <!--/ Abstract. -->

  </div>

    <!-- Paper video. -->
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <!-- <iframe src="https://www.youtube.com/embed/UdzoagBgWTA?rel=0&amp;showinfo=0" -->
          <iframe src="https://www.youtube.com/embed/sguEFlVdEUA?si=50Ng-vZSNoTWdJZE"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/sguEFlVdEUA?si=50Ng-vZSNoTWdJZE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- <h2 class="title is-3"><span class="dcliport">Tasks</span></h2>
          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                  <div class="item item-steve">
                    <video poster="" id="steve" autoplay controls muted loop height="100%">
                      <source src="https://shengqiang-zhang.github.io/lohoravens-webpage/media/videos/1_folding.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-fullbody">
                    <video poster="" id="fullbody" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/4_chess.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-shiba">
                    <video poster="" id="shiba" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/3_packing.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-blueshirt">
                    <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/6_sweeping.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-shiba">
                    <video poster="" id="shiba" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/9_cherry.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-shiba">
                    <video poster="" id="shiba" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/7_reading.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                <div class="item item-chair-tp">
                    <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/2_bowl.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-blueshirt">
                    <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/8_rope.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                  <div class="item item-blueshirt">
                    <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
                      <source src="https://cliport.github.io/media/videos/5_stacking.mp4"
                              type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
            </div>
          </section>
          <h2 class="subtitle has-text-centered">
          </br>
            We learn <b>one multi-task policy</b> for 9 real-world tasks including folding cloths, sweeping beans etc. with just <b>179</b> image-action training pairs.
          </h2> -->

        <h2 class="title is-3"><span class="dcliport">Baselines</span></h2>
        <div class="content has-text-justified">
          <p>
            It has been a mainstream method to use LLMs as the planner for a robot's execution.  
            However, how to incorporate real-time visual observation feedback into the
            LLM's input is still an under-explored problem.  
            This modality gap is especially severe for long-horizon robotic tasks because
            an execution error in each of the robot's steps can affect all the following steps.
            To solve this modality bridging problem, we propose two baseline methods to translate the visual observation into feedback that the LLM can understand for its closed-loop planning. 
            We use the Planner-Actor-Reporter paradigm to unify our two baselines.
            The feedback generation models of the two baselines are working as the Reporter module.
          </p>
        </div>


        <!-- Interpolating. -->
        <h3 class="title is-4">Explicit feedback: Caption generation</h3>
        <img src="https://shengqiang-zhang.github.io/lohoravens-webpage/media/images/explicit_baseline.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        <br/>
        <div class="content has-text-justified">
          <p>
            Inner Monologue demonstrated that human-provided language feedback can significantly improve high-level instruction completion on robotic manipulation tasks. 
            But human-written language feedback is too expensive to scale.  We therefore explore a caption generation based model as an automatic way to generate language feedback without training.
          </p>

          <p>
          As shown in the above figure, we use Llama 2 and the trained pick-and-place CLIPort primitive as the Planner and Actor, respectively.
          For the Reporter,
          we use the VLM OpenFlamingo with few-shot prompting to generate the following two types of feedback:
          <i>Observation state feedback</i>
          which is the information about the objects on the table and their potential changes,
          and 
          <i>Action success state feedback</i>
          which is the description whether the last instruction is executed successfully or not.
          </p>

          <p>
          When a step's action has executed, there will be a top-down RGB image rendered by the simulator. 
          The VLM as the Reporter module will generate the caption feedback based on the current image or the whole image history.
          This caption feedback is sent to the LLM for its next-step planning.
          The Planner-Actor-Reporter closed-loop process will be
          iteratively executed until the high-level goal is achieved
          or the maximum number of trial steps has been exceeded.
          </p>
          
            
        </div>
        <!-- <br/>
            <b>Paradigm 1:</b> Unlike existing object detectors, CLIP is not limited to a predefined set of object classes. And unlike other vision-language models, it's not restricted by a top-down pipeline that detects objects with bounding boxes or instance segmentations. This allows us to forgo the traditional paradigm of training explicit detectors for cloths, pliers, chessboard squares, cherry stems, and other arbitrary things. 
        <br/> -->
        <br/>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Implicit feedback: Learnable interface</h3>
        <img src="https://shengqiang-zhang.github.io/lohoravens-webpage/media/images/implicit_baseline.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        <div class="content has-text-justified">
          <p>
            We use this two-stream architecture in all three networks of <a target=”_blank” href="https://transporternets.github.io/">TransporterNets</a> 
            to predict pick and place affordances at each timestep. TransporterNets first attends to a local region to decide where to pick, 
            then computes a placement location by finding the best match for the picked region through 
            cross-correlation of deep visual features. This structure serves as a powerful inductive bias for learning <a target="_blank" href="https://fabianfuchsml.github.io/equivariance1of2/">roto-translationally equivariant</a> representations in tabletop environments.

          </p>
        </div>
        <div class="content has-text-centered">
          <video id="transporter-gif"
                 controls
                 muted
                 autoplay
                 loop
                 width="40%">
            <source src="https://transporternets.github.io/images/animation.mp4"
                    type="video/mp4">
          </video>
          <p>
          Credit: <a href="https://transporternets.github.io/">Zeng et. al (Google)</a>
          </p>
        </div>
        <br/>
            <b>Paradigm 2:</b> TransporterNets takes an <a target="_blank" href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> to perception where the objective is to <i>detect actions</i> rather than <i>detect objects</i> and then learn a policy. Keeping the action-space grounded in the perceptual input allows us to exploit geometric symmetries for efficient representation learning. 
            When combined with CLIP's pre-trained representations, this enables the learning of reusable manipulation skills without any "objectness" assumptions.
        <br/>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3"><span class="dcliport">Results</span></h2>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Single-Task Models</h3>

            Trained with
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="1">1</option>
              <option value="10">10</option>
              <option value="100">100</option>
              <option value="1000" selected="selected">1000</option>
              </select>
            </div>
            demos, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="align-rope">align-rope</option>
              <option value="assembling-kits-seq-seen-colors">assembling-kits-seq-seen-colors</option>
              <option value="assembling-kits-seq-unseen-colors">assembling-kits-seq-unseen-colors</option>
              <option value="packing-boxes-pairs-seen-colors">packing-boxes-pairs-seen-colors</option>
              <option value="packing-boxes-pairs-unseen-colors">packing-boxes-pairs-unseen-colors</option>
              <option value="packing-seen-google-objects-seq" selected="selected">packing-seen-google-objects-seq</option>
              <option value="packing-unseen-google-objects-seq">packing-unseen-google-objects-seq</option>
              <option value="packing-seen-google-objects-group">packing-seen-google-objects-group</option>
              <option value="packing-unseen-google-objects-group">packing-unseen-google-objects-group</option>
              <option value="packing-shapes">packing-shapes</option>
              <option value="put-block-in-bowl-seen-colors">put-block-in-bowl-seen-colors</option>
              <option value="put-block-in-bowl-unseen-colors">put-block-in-bowl-unseen-colors</option>
              <option value="separating-piles-seen-colors">separating-piles-seen-colors</option>
              <option value="separating-piles-unseen-colors">separating-piles-unseen-colors</option>
              <option value="stack-block-pyramid-seq-seen-colors">stack-block-pyramid-seq-seen-colors</option>
              <option value="stack-block-pyramid-seq-unseen-colors">stack-block-pyramid-seq-unseen-colors</option>
              <option value="towers-of-hanoi-seq-seen-colors">towers-of-hanoi-seq-seen-colors</option>
              <option value="towers-of-hanoi-seq-unseen-colors">towers-of-hanoi-seq-unseen-colors</option>
              </select>
            </div>
            instance
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="01">01</option>
              <option value="02">02</option>
              <option value="03">03</option>
              <option value="04">04</option>
              <option value="05" selected="selected">05</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="single-task-result-video"
                   controls
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="https://cliport.github.io/media/results_web/packing-seen-google-objects-seq-two_stream_full_clip_lingunet_lat_transporter-n1000-train/videos/packing-seen-google-objects-seq-000005.mp4"
                      type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h3 class="title is-5">One Multi-Task Model</h3>
            
            Trained with
            <div class="select is-small">
              <select id="multi-menu-demos" onchange="updateMultiVideo()">
              <option value="1">1 T</option>
              <option value="10">10 T</option>
              <option value="100">100 T</option>
              <option value="1000" selected="selected">1000 T</option>
              </select>
            </div>
            demos, evaluated on  
            <div class="select is-small">   
              <select id="multi-menu-tasks" onchange="updateMultiVideo()">
              <option value="align-rope">align-rope</option>
              <option value="assembling-kits-seq-seen-colors">assembling-kits-seq-seen-colors</option>
              <option value="assembling-kits-seq-unseen-colors">assembling-kits-seq-unseen-colors</option>
              <option value="packing-boxes-pairs-seen-colors" selected="selected">packing-boxes-pairs-seen-colors</option>
              <option value="packing-boxes-pairs-unseen-colors">packing-boxes-pairs-unseen-colors</option>
              <option value="packing-seen-google-objects-seq">packing-seen-google-objects-seq</option>
              <option value="packing-unseen-google-objects-seq">packing-unseen-google-objects-seq</option>
              <option value="packing-seen-google-objects-group">packing-seen-google-objects-group</option>
              <option value="packing-unseen-google-objects-group">packing-unseen-google-objects-group</option>
              <option value="packing-shapes">packing-shapes</option>
              <option value="put-block-in-bowl-seen-colors">put-block-in-bowl-seen-colors</option>
              <option value="put-block-in-bowl-unseen-colors">put-block-in-bowl-unseen-colors</option>
              <option value="separating-piles-seen-colors">separating-piles-seen-colors</option>
              <option value="separating-piles-unseen-colors">separating-piles-unseen-colors</option>
              <option value="stack-block-pyramid-seq-seen-colors">stack-block-pyramid-seq-seen-colors</option>
              <option value="stack-block-pyramid-seq-unseen-colors">stack-block-pyramid-seq-unseen-colors</option>
              <option value="towers-of-hanoi-seq-seen-colors">towers-of-hanoi-seq-seen-colors</option>
              <option value="towers-of-hanoi-seq-unseen-colors">towers-of-hanoi-seq-unseen-colors</option>
              </select>
            </div>
            instance
            <div class="select is-small">
              <select id="multi-menu-instances" onchange="updateMultiVideo()">
              <option value="01">01</option>
              <option value="02">02</option>
              <option value="03">03</option>
              <option value="04" selected="selected">04</option>
              <option value="05">05</option>
              </select>
            </div>
            </br>
            </br>

            <video id="multi-task-result-video"
                   controls
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="https://cliport.github.io/media/results_web/packing-boxes-pairs-seen-colors-two_stream_full_clip_lingunet_lat_transporter-n1000-train/videos/multi-language-conditioned-packing-boxes-pairs-seen-colors-000004.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>

        <h3 class="title is-4">Affordance Predictions</h3>
        <div class="content has-text-justified">
          <p>
            Examples of pick and place affordance predictions from multi-task <span class="dcliport">CLIPort</span> models:
          </p>
        </div>
        <br/>
        <img src="https://cliport.github.io/media/images/affordances.png" class="interpolation-image" 
         alt="Interpolate start reference image."/>
        <br/>
        <br/>
        <img src="https://cliport.github.io/media/images/affordance2.png" class="interpolation-image" 
         alt="Interpolate start reference image."/>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2021cliport,
  title     = {CLIPort: What and Where Pathways for Robotic Manipulation},
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 5th Conference on Robot Learning (CoRL)},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
